---
title: "Linear models"
author: "tommaso tarchi"
date: "2023-01-17"
output: 
  html_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
## importing the dataset and omitting the missing values
datafile <- read.csv("../WiscNursingHome.csv")
datafile <- na.omit(datafile)

## dividing data from different years
data_2000 <- datafile[datafile$CRYEAR == 2000, ]
rownames(data_2000) <- 1:nrow(data_2000)
data_2001 <- datafile[datafile$CRYEAR == 2001, ]
rownames(data_2001) <- 1:nrow(data_2001)

datafile$ORGSTR <- as.factor(datafile$ORGSTR)
datafile$MSA <- as.factor(datafile$MSA)
```

```{r}
library(ggplot2)
library(GGally)
```

For the moment we just work on the data coming from the 2000 survey, to avoid dependency between observations.


## Quantitative variables only

First we consider the quantitative variables only.

Let's see the correlation:
```{r}
ggpairs(subset(data_2000, select = c(SQRFOOT, NUMBED, TPY)))
```

We can see the strong correlation between all the three variables. In particular we see the almost perfectly linear correlation between TPY and NUMBED.


### Simple linear models

Being NUMBED the most linearly correlated variable w.r.t. PTY, we start modelling PTY using NUMBED and subsequently add SQRFOOT and the interaction between the two:
```{r}
summary(lm(TPY ~ NUMBED + SQRFOOT + NUMBED:SQRFOOT, data = data_2000))
```

The t test related to SQRFOOT does not give enough evidence against the null hypotesis. To understand better the role of the variable we can perform the analysis of variance:
```{r}
anova(glm(TPY ~ NUMBED + SQRFOOT + NUMBED:SQRFOOT, data = data_2000, family = gaussian),
      test = "Chisq")
```

As we can see, despite being NUMBED and SQRFOOT highly correlated, it is worth including the second variable to the model too. Adding the interaction, instead, seems not to give any important information to the model.

We can also use Akaike information criterium to check this result (here we compare some of the possible combinations):
```{r}
AIC(lm(TPY ~ NUMBED, data = data_2000),
    lm(TPY ~ SQRFOOT, data = data_2000),
    lm(TPY ~ NUMBED + SQRFOOT, data = data_2000),
    lm(TPY ~ NUMBED:SQRFOOT, data = data_2000),
    lm(TPY ~ NUMBED + SQRFOOT + NUMBED:SQRFOOT, data = data_2000),
    lm(TPY ~ NUMBED + NUMBED:SQRFOOT, data = data_2000),
    lm(TPY ~ SQRFOOT + NUMBED:SQRFOOT, data = data_2000))
```

Again the analysis indicates NUMBED as the most relevant variable and the interaction between NUMBED and SQRFOOT as basically non relevant.

### Models with log-transformed predictor

We can try to improve the model by log-transforming the predictors:
```{r}
AIC(lm(TPY ~ log(NUMBED), data = data_2000),
    lm(TPY ~ log(SQRFOOT), data = data_2000),
    lm(TPY ~ log(NUMBED) + log(SQRFOOT), data = data_2000),
    lm(TPY ~ log(NUMBED) + SQRFOOT, data = data_2000),
    lm(TPY ~ NUMBED + log(SQRFOOT), data = data_2000))
```

We cannot see any improvement.


### Graphical analysis

At the moment, the best model (according to AIC) seems to be the one with just the single linear contributions of the two variables. Let's analyze the residuals:
```{r}
fit.linear <- lm(TPY ~ NUMBED + SQRFOOT, data = data_2000)
summary(fit.linear)
```
```{r}
par(mfrow = c(2, 2))
plot(fit.linear)
```

The residuals are not exactly as we would expect from a good linear model. In particular there seem to be a couple of outliers, the residuals are not normally distributed on the edges and homoscedasticity is not satisfied.

Let's also try to inspect the residuals' plots for the other good models (according to AIC) we got previously:
```{r}
par(mfrow = c(2, 2))
plot(lm(TPY ~ NUMBED, data = data_2000))
```
```{r}
par(mfrow = c(2, 2))
plot(lm(TPY ~ NUMBED + SQRFOOT + NUMBED:SQRFOOT, data = data_2000))
```
```{r}
par(mfrow = c(2, 2))
plot(lm(TPY ~ NUMBED + NUMBED:SQRFOOT, data = data_2000))
```
```{r}
par(mfrow = c(2, 2))
plot(lm(TPY ~ NUMBED + log(SQRFOOT), data = data_2000))
```

None of the models above seems to satisfy the assumptions on the linear model.

Let's now look at the distribution of the data:
```{r}
par(mfrow = c(2, 2))
hist(data_2000$TPY, xlab = "TPY", freq = TRUE)
hist(data_2000$NUMBED, xlab = "NUMBED", freq = TRUE)
hist(data_2000$SQRFOOT, xlab = "SQRFOOT", freq = TRUE)
```

As we can see all of the variables are strongly skewed.


### Models with log-transformed TPY

First let's see the correlation:
```{r}
## fare ggpairs() con il log delle variabili
```

We see that also TPY is strongly skewed, therefore we can try to model its log-transform:
```{r}
summary(lm(log(TPY) ~ log(NUMBED) + log(SQRFOOT) + log(NUMBED):log(SQRFOOT), data = data_2000))
```

```{r}
anova(glm(log(TPY) ~ log(NUMBED) + log(SQRFOOT) + log(NUMBED):log(SQRFOOT), data = data_2000,
          family = gaussian), test = "Chisq")
```

In this case even the main effect of SQRFOOT seems to be non relevant. Let's give a look at the plot of the model with just log(NUMBED) and with just log(SQRFOOT):
```{r}
summary(lm(log(TPY) ~ log(NUMBED), data = data_2000))
```
```{r}
summary(lm(log(TPY) ~ log(SQRFOOT), data = data_2000))
```

And to their residuals:
```{r}
par(mfrow = c(2, 2))
plot(lm(log(TPY) ~ log(NUMBED), data = data_2000))
```
```{r}
par(mfrow = c(2, 2))
plot(lm(log(TPY) ~ log(SQRFOOT), data = data_2000))
```

We look at the AIC too:
```{r}
AIC(lm(log(TPY) ~ log(NUMBED), data = data_2000),
    lm(log(TPY) ~ log(SQRFOOT), data = data_2000),
    lm(log(TPY) ~ log(NUMBED) + log(SQRFOOT), data = data_2000))
```

Again NUMBED seems to be a good predictor, but the residuals suggest that the assumption on the linear model are not satisfied.

On the other hand SQRFOOT seems to be less relevant but with mush better residuals (apart from the high leverage outlier 331).

We can also try a kind of mixed model:
```{r}
summary(lm(log(TPY) ~ NUMBED + log(SQRFOOT), data = data_2000))
```
```{r}
par(mfrow = c(2, 2))
plot(lm(log(TPY) ~ NUMBED + log(SQRFOOT), data = data_2000))
```
```{r}
AIC(lm(log(TPY) ~ NUMBED + log(SQRFOOT), data = data_2000))
```

But the results are not encouraging.

```{r}
summary(lm(log(TPY) ~ SQRFOOT + NUMBED, data = data_2000))
```
```{r}
summary(lm(log(TPY) ~ log(SQRFOOT) + log(NUMBED), data = data_2000))
```



### Models with inverse and sqrt trasformed TPY

To fix residuals we can also try:
```{r}
summary(lm(I(1/TPY) ~ NUMBED + SQRFOOT, data = data_2000))
```
```{r}
summary(lm(I(sqrt(TPY)) ~ NUMBED + SQRFOOT, data = data_2000))
```

```{r}
par(mfrow = c(2, 2))
plot(lm(I(1/TPY) ~ NUMBED + SQRFOOT, data = data_2000))
```
```{r}
par(mfrow = c(2, 2))
plot(lm(I(sqrt(TPY)) ~ NUMBED + SQRFOOT, data = data_2000))
```

the results are even worse than before.


## Adding categorical variables

By using the numerical variables only we were not able to obtain a good model. In particular it was not possible to build a model in which all the characteristic assumptions for linear models were met. We can then try to improve our model by adding the categorical predictors.

First of all we can observe two things about the categorical variables:

- MSA specifies the information given by URBAN (therefore they will be alternative variables in the model);

- Information expressed by PRO e TAXEXEMPT is implicit in ORGSTR.

We can also visualize the previous two observations by graphs:
```{r}
### roba fatta da Simone
```


For the moment let's exclude the variable MCERT and use the most "generic" variables. As a baseline we take the linear model with the higher AIC score.
```{r}
summary(lm(TPY ~ NUMBED + SQRFOOT + TAXEXEMPT + SELFFUNDINS + URBAN + PRO, data = data_2000))
```
```{r}
anova(glm(TPY ~ NUMBED + SQRFOOT + TAXEXEMPT + SELFFUNDINS + URBAN + PRO, data = data_2000,
          family = gaussian), test = "Chisq")
```
```{r}
par(mfrow = c(2, 2))
plot(lm(TPY ~ NUMBED + SQRFOOT + TAXEXEMPT + SELFFUNDINS + URBAN + PRO,
        data = data_2000))
```

Let's now try with the log-transformed variables:
```{r}
summary(lm(log(TPY) ~ log(NUMBED) + log(SQRFOOT) + TAXEXEMPT + SELFFUNDINS + URBAN + PRO,
           data = data_2000))
```
```{r}
anova(glm(log(TPY) ~ log(NUMBED) + log(SQRFOOT) + TAXEXEMPT + SELFFUNDINS + URBAN + PRO,
          data = data_2000, family = gaussian), test = "Chisq")
```
```{r}
par(mfrow = c(2, 2))
plot(lm(log(TPY) ~ log(NUMBED) + log(SQRFOOT) + TAXEXEMPT + SELFFUNDINS + URBAN + PRO,
        data = data_2000))
```

Better, but still the outliers are a problem.

Notice that TAXEXEMPT seems to be the only relevant categorical variable. let's try to put it as last to check:
```{r}
anova(glm(log(TPY) ~ log(NUMBED) + log(SQRFOOT) + SELFFUNDINS + URBAN + PRO + TAXEXEMPT,
          data = data_2000, family = gaussian), test = "Chisq")
```

The only categorical variables that seem to be significant are then PRO and TAXEXEMPT.


## Best model

In the end the only acceptable model (that is: the only model whose residuals' plots confirm that the linear models' hypoteses are met) seems to be the following.
```{r}
fit.best <- lm(log(TPY) ~ log(SQRFOOT) + TAXEXEMPT + PRO, data = data_2000)
summary(fit.best)
```
```{r}
anova(fit.best, test = "Chisq")
```

```{r}
par(mfrow = c(2, 2))
plot(fit.best)
```

The plots are not bad, but we have a couple of outliers. Let's remove them:
```{r}
data_mod <- data_2000[-331, ]
data_mod <- data_mod[-326, ]
rownames(data_2000) <- 1:nrow(data_2000)

fit.best.mod <- lm(log(TPY) ~ log(SQRFOOT), data = data_mod)
summary(fit.best.mod)
```
```{r}
par(mfrow = c(2, 2))
plot(fit.best.mod)
```


***Capire se TAXEXEMPT e PRO sono correlate. In tal caso basta metterne una***